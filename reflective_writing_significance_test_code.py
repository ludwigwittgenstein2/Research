# -*- coding: utf-8 -*-
"""Reflective_Writing_Significance_Test

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1quTUdCbxlJ8GeWXNYzQ4N0WQ5hw8z1mt
"""

# Conducting Statistical Significance Test

# What is it?
# Wilcoxon signed-rank test is a non-parametric rank test for statistical hypothesis
# testing used either to test the location of a population based on a sample of data,
# or to compare the locations of two populations using two matched samples

!pip install scipy

# Scores:
# First Pass:
# Clinical Scores:[5.0, 4.5, 4.0, 4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 4.5]
# Reflective Scores: [4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 5.0, 4.5, 4.0, 4.0]

# Second Pass:
#clinical_scores = [4.5, 4.5, 4.5, 5.0, 5.0, 4.5, 4.5, 4.5, 4.5, 4.5]
#reflective_scores = [4.5, 4.5, 4.5, 5.0, 5.0, 4.5, 4.5, 4.5, 4.0, 4.5]

#Third Pass:
# clinical_scores = [4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 5.0, 4.5, 4.5, 4.5]
# reflective_scores = [5.0, 4.5, 4.0, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5]

# First run

# 1. Import the necessary function from the scipy library
from scipy.stats import wilcoxon

# 2. Define your two lists of paired scores
#First Run
clinical_scores = [5.0, 4.5, 4.0, 4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 4.5]
reflective_scores = [4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 5.0, 4.5, 4.0, 4.0]

# 3. Perform the Wilcoxon Signed-Rank Test
# The function takes your two lists as input
# It returns two values: the test statistic and the p-value
stat, p_value = wilcoxon(clinical_scores, reflective_scores)

# 4. Print and interpret the results
print(f"Wilcoxon W-statistic: {stat:.1f}")
print(f"P-value: {p_value:.3f}")

# The p-value is the most important part for interpretation
if p_value < 0.05:
    print("The difference between the rubrics is statistically significant.")
else:
    print("There is no significant difference between the rubrics.")

#Second Pass

# 1. Import the necessary function from the scipy library
from scipy.stats import wilcoxon

# 2. Define your two lists of paired scores
#First Run
clinical_scores = [4.5, 4.5, 4.5, 5.0, 5.0, 4.5, 4.5, 4.5, 4.5, 4.5]
reflective_scores = [4.5, 4.5, 4.5, 5.0, 5.0, 4.5, 4.5, 4.5, 4.0, 4.5]

# 3. Perform the Wilcoxon Signed-Rank Test
# The function takes your two lists as input
# It returns two values: the test statistic and the p-value
stat, p_value = wilcoxon(clinical_scores, reflective_scores)

# 4. Print and interpret the results
print(f"Wilcoxon W-statistic: {stat:.1f}")
print(f"P-value: {p_value:.3f}")

# The p-value is the most important part for interpretation
if p_value < 0.05:
    print("The difference between the rubrics is statistically significant.")
else:
    print("There is no significant difference between the rubrics.")

#Third Pass:

# Importing the necessary function from the scipy library
from scipy.stats import wilcoxon

# Defining our two lists of paired scores
#First Run
clinical_scores = [4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 5.0, 4.5, 4.5, 4.5]
reflective_scores = [5.0, 4.5, 4.0, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5]

# 3. Perform the Wilcoxon Signed-Rank Test
# The function takes your two lists as input
# It returns two values: the test statistic and the p-value
stat, p_value = wilcoxon(clinical_scores, reflective_scores)

# 4. Print and interpret the results
print(f"Wilcoxon W-statistic: {stat:.1f}")
print(f"P-value: {p_value:.3f}")

# The p-value is the most important part for interpretation
if p_value < 0.05:
    print("The difference between the rubrics is statistically significant.")
else:
    print("There is no significant difference between the rubrics.")

#Friedman Test

import numpy as np
from scipy.stats import friedmanchisquare

# Data for 10 subjects across 6 conditions (Clinical/Reflective scores for P1, P2, and P3).
# The observations are paired by index (index 0 is Subject 1 across all lists, etc.)

# First Pass (P1)
p1_clinical = [5.0, 4.5, 4.0, 4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 4.5]
p1_reflective = [4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 5.0, 4.5, 4.0, 4.0]

# Second Pass (P2)
p2_clinical = [4.5, 4.5, 4.5, 5.0, 5.0, 4.5, 4.5, 4.5, 4.5, 4.5]
p2_reflective = [4.5, 4.5, 4.5, 5.0, 5.0, 4.5, 4.5, 4.5, 4.0, 4.5]

# Third Pass (P3)
p3_clinical = [4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 5.0, 4.5, 4.5, 4.5]
p3_reflective = [5.0, 4.5, 4.0, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5]

print("--- Friedman Test Analysis (Comparing 6 Conditions) ---")
print(f"Number of subjects (N): {len(p1_clinical)}")
print("Conditions (k=6): P1 Clinical, P1 Reflective, P2 Clinical, P2 Reflective, P3 Clinical, P3 Reflective\n")

# Perform the Friedman test
# H0: The median scores are the same across all 6 conditions.
# Ha: At least one median score is different.
stat, p_value = friedmanchisquare(
    p1_clinical, p1_reflective,
    p2_clinical, p2_reflective,
    p3_clinical, p3_reflective
)

print(f"Friedman Chi-squared Statistic: {stat:.4f}")
print(f"P-value: {p_value:.4f}")

# Interpretation based on common alpha = 0.05
alpha = 0.05
print(f"\nSignificance Level (alpha): {alpha}")

if p_value < alpha:
    print("\nConclusion: The P-value is less than the significance level.")
    print("We reject the null hypothesis ($H_0$). There is a statistically significant difference in median scores among the six conditions.")
    print("If you reject $H_0$, you would typically follow up with post-hoc tests (like Nemenyi's test) to find out which specific conditions differ.")
else:
    print("\nConclusion: The P-value is greater than the significance level.")
    print("We fail to reject the null hypothesis ($H_0$). There is no statistically significant evidence of a difference in median scores among the six conditions.")

# Optional: Print mean rank of each group (a necessary step for the test)
data = np.array([p1_clinical, p1_reflective, p2_clinical, p2_reflective, p3_clinical, p3_reflective]).T
# Rank the scores for each subject (row)
ranks = np.apply_along_axis(lambda x: np.argsort(np.argsort(x)) + 1, 1, data)
mean_ranks = np.mean(ranks, axis=0)
print("\nMean Ranks (Lower Rank = Lower Median Score):")
print(f"P1 Clinical Rank: {mean_ranks[0]:.2f}")
print(f"P1 Reflective Rank: {mean_ranks[1]:.2f}")
print(f"P2 Clinical Rank: {mean_ranks[2]:.2f}")
print(f"P2 Reflective Rank: {mean_ranks[3]:.2f}")
print(f"P3 Clinical Rank: {mean_ranks[4]:.2f}")
print(f"P3 Reflective Rank: {mean_ranks[5]:.2f}")



from scipy.stats import binomtest

def sign_test(clinical_scores, reflective_scores, alternative='two-sided'):
    if len(clinical_scores) != len(reflective_scores):
        raise ValueError("Score lists must be of equal length.")

    positive_diff = 0
    negative_diff = 0
    for c, r in zip(clinical_scores, reflective_scores):
        if c > r:
            positive_diff += 1
        elif c < r:
            negative_diff += 1
    n = positive_diff + negative_diff
    x = positive_diff

    result = binomtest(x, n, 0.5, alternative=alternative)
    return {
        'positive_diff': positive_diff,
        'negative_diff': negative_diff,
        'n_non_ties': n,
        'p_value': result.pvalue
    }

# Scores for first, second, and third pass
first_pass_clinical = [5.0, 4.5, 4.0, 4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 4.5]
first_pass_reflective = [4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 5.0, 4.5, 4.0, 4.0]

second_pass_clinical = [4.5, 4.5, 4.5, 5.0, 5.0, 4.5, 4.5, 4.5, 4.5, 4.5]
second_pass_reflective = [4.5, 4.5, 4.5, 5.0, 5.0, 4.5, 4.5, 4.5, 4.0, 4.5]

third_pass_clinical = [4.5, 4.5, 4.0, 4.5, 4.5, 4.5, 5.0, 4.5, 4.5, 4.5]
third_pass_reflective = [5.0, 4.5, 4.0, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5]

# Run the sign test on all three passes
print("First Pass:", sign_test(first_pass_clinical, first_pass_reflective))
print("Second Pass:", sign_test(second_pass_clinical, second_pass_reflective))
print("Third Pass:", sign_test(third_pass_clinical, third_pass_reflective))

#Overall conclusion: with these data, we find no evidence of a rubric effect.
#
#

# Cochran's Q test for your rubric data
# -------------------------------------
# What it tests here:
#   For each student in each run:
#       y = 1 if reflective_rubric > clinical_rubric, else 0 (ties -> 0)
#   Test whether the proportion of y=1 differs across runs (First/Second/Third),
#   for the SAME students observed under all runs.

from io import StringIO
import numpy as np
import pandas as pd
from scipy.stats import chi2

RAW = """Run Student Year Rubric Score
First 1 1 reflective_rubric 5
First 1 1 clinical_rubric 4.5
First 2 1 reflective_rubric 4.5
First 2 1 clinical_rubric 4.5
First 3 1 reflective_rubric 4.5
First 3 1 clinical_rubric 4
First 4 1 clinical_rubric 4.5
First 5 4 reflective_rubric 4.5
First 5 4 clinical_rubric 4.5
First 6 4 reflective_rubric 4
First 6 4 clinical_rubric 4
First 7 4 reflective_rubric 4.5
First 7 4 clinical_rubric 4.5
First 8 4 reflective_rubric 5
First 8 4 clinical_rubric 4.5
First 9 4 reflective_rubric 4
First 9 4 clinical_rubric 3.5
First 10 4 reflective_rubric 4.5
First 10 4 clinical_rubric 4
Second 1 4 reflective_rubric 4.5
Second 1 4 clinical_rubric 4.5
Second 2 4 reflective_rubric 4.5
Second 2 4 clinical_rubric 4.5
Second 3 4 reflective_rubric 4.5
Second 3 4 clinical_rubric 4.5
Second 4 4 reflective_rubric 5
Second 4 4 clinical_rubric 4
Second 5 4 reflective_rubric 4.5
Second 5 4 clinical_rubric 5
Second 6 4 reflective_rubric 4.5
Second 6 4 clinical_rubric 4.5
Second 7 4 reflective_rubric 4.5
Second 7 4 clinical_rubric 5
Second 8 4 reflective_rubric 4.5
Second 8 4 clinical_rubric 4.5
Second 9 4 reflective_rubric 4
Second 9 4 clinical_rubric 4
Second 10 4 reflective_rubric 4.5
Second 10 4 clinical_rubric 5
Third 1 1 reflective_rubric 4.5
Third 1 1 clinical_rubric 4.5
Third 2 4 reflective_rubric 4.5
Third 2 4 clinical_rubric 5
Third 3 4 reflective_rubric 4
Third 3 4 clinical_rubric 4.5
Third 4 4 reflective_rubric 5
Third 4 4 clinical_rubric 4.5
Third 5 4 reflective_rubric 4.5
Third 5 4 clinical_rubric 5
Third 6 4 reflective_rubric 4.5
Third 6 4 clinical_rubric 4.5
Third 7 4 reflective_rubric 4.5
Third 7 4 clinical_rubric 5
Third 8 4 reflective_rubric 4.5
Third 8 4 clinical_rubric 4.5
Third 9 4 reflective_rubric 4
Third 9 4 clinical_rubric 4
Third 10 4 reflective_rubric 4.5
Third 10 4 clinical_rubric 5
"""

def build_binary_matrix(df: pd.DataFrame, runs=("First","Second","Third")) -> pd.DataFrame:
    """Return X: rows = students present in all runs; cols = runs; values in {0,1}
       where 1 = reflective_rubric > clinical_rubric, else 0 (ties -> 0)."""
    wide = (
        df.pivot_table(index=["Run","Student"], columns="Rubric", values="Score", aggfunc="first")
          .reset_index()
          .dropna(subset=["reflective_rubric", "clinical_rubric"])
    )
    wide["ref_gt_cli"] = (wide["reflective_rubric"] > wide["clinical_rubric"]).astype(int)

    # Keep only students who appear (with both rubrics) in *all* runs
    students_all = set(wide["Student"].unique())
    for r in runs:
        students_all &= set(wide.loc[wide["Run"]==r, "Student"].unique())
    students_all = sorted(students_all)

    X = pd.DataFrame(index=students_all, columns=runs, dtype=int)
    for r in runs:
        sub = wide.loc[wide["Run"]==r, ["Student","ref_gt_cli"]].set_index("Student")["ref_gt_cli"]
        X[r] = sub.reindex(students_all).astype(int)
    return X

def cochrans_q_manual(X: pd.DataFrame):
    """Compute Cochran's Q statistic and chi-square p-value (df=k-1)."""
    X = X.astype(int)
    k = X.shape[1]
    C = X.sum(axis=0).to_numpy()   # column totals (successes per run)
    R = X.sum(axis=1).to_numpy()   # row totals (successes per student)
    T = C.sum()

    numerator = (k - 1) * (k * np.sum(C**2) - T**2)
    denominator = (k * T - np.sum(R**2))
    if denominator == 0:
        return np.nan, k - 1, np.nan
    Q = numerator / denominator
    p = 1.0 - chi2.cdf(Q, df=k-1)
    return float(Q), k-1, float(p)

# ---- Run it ----
df = pd.read_csv(StringIO(RAW), sep=r"\s+")
runs = ("First","Second","Third")
X = build_binary_matrix(df, runs)

print("Binary table (1 = reflective > clinical, 0 = otherwise; ties -> 0)")
print(X.to_string(), "\n")

Q, df_q, p = cochrans_q_manual(X)
print(f"Cochran's Q (manual): Q = {Q:.4f}, df = {df_q}, p = {p:.6f}")

# Optional cross-check with statsmodels, if installed
try:
    from statsmodels.stats.contingency_tables import cochrans_q
    sm_res = cochrans_q(X.values)
    print(f"[statsmodels] Q = {float(sm_res.statistic):.4f}, p = {float(sm_res.pvalue):.6f}")
except Exception as e:
    print("[statsmodels] not available; manual result shown above.")

import pandas as pd
from itertools import combinations
from statsmodels.stats.contingency_tables import mcnemar
from statsmodels.stats.multitest import multipletests

# X is the binary table you printed (rows=students in all runs; cols=["First","Second","Third"])

def pairwise_mcnemar_with_holm(X, exact=True, correction=False):
    pairs, pvals = [], []
    for a, b in combinations(X.columns, 2):
        # 2x2 table of matched outcomes across runs a vs b
        tab = pd.crosstab(X[a], X[b]).reindex(index=[0,1], columns=[0,1], fill_value=0)
        res = mcnemar(tab, exact=exact, correction=correction)  # exact recommended with small counts
        pairs.append((a, b, tab.loc[1,0], tab.loc[0,1]))  # discordant counts b=1,0 and c=0,1
        pvals.append(res.pvalue)
    # Holm correction
    reject, p_holm, _, _ = multipletests(pvals, method="holm")
    out = []
    for (a,b,b10,c01), p_raw, p_adj, rej in zip(pairs, pvals, p_holm, reject):
        out.append({"contrast": f"{a} vs {b}",
                    "discordant b(1,0)": int(b10), "discordant c(0,1)": int(c01),
                    "p_raw": float(p_raw), "p_holm": float(p_adj), "reject@0.05": bool(rej)})
    return pd.DataFrame(out)

posthoc = pairwise_mcnemar_with_holm(X)
print(posthoc)



import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (16, 12)

# Create figure with subplots
fig = plt.figure(figsize=(16, 12))
gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)

# Color scheme
colors = ['#2ecc71', '#e74c3c', '#3498db', '#f39c12', '#9b59b6', '#1abc9c']

# ========== 1. Wilcoxon Test Results (3 Passes) ==========
ax1 = fig.add_subplot(gs[0, :2])

passes = ['First Pass', 'Second Pass', 'Third Pass']
w_stats = [10.5, 0.0, 1.5]
p_values_wilcoxon = [1.000, 1.000, 1.000]

x = np.arange(len(passes))
width = 0.35

bars1 = ax1.bar(x - width/2, w_stats, width, label='W-statistic', color='#3498db', alpha=0.7)
ax1_twin = ax1.twinx()
bars2 = ax1_twin.bar(x + width/2, p_values_wilcoxon, width, label='P-value', color='#e74c3c', alpha=0.7)

ax1.set_xlabel('Pass', fontsize=12, fontweight='bold')
ax1.set_ylabel('W-statistic', fontsize=11, fontweight='bold', color='#3498db')
ax1_twin.set_ylabel('P-value', fontsize=11, fontweight='bold', color='#e74c3c')
ax1.set_title('Wilcoxon Signed-Rank Test Results Across Passes', fontsize=14, fontweight='bold', pad=20)
ax1.set_xticks(x)
ax1.set_xticklabels(passes)
ax1.tick_params(axis='y', labelcolor='#3498db')
ax1_twin.tick_params(axis='y', labelcolor='#e74c3c')
ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.3)
ax1_twin.axhline(y=0.05, color='red', linestyle='--', alpha=0.5, label='α=0.05')

# Add value labels
for bar in bars1:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.1f}', ha='center', va='bottom', fontsize=9)

for bar in bars2:
    height = bar.get_height()
    ax1_twin.text(bar.get_x() + bar.get_width()/2., height,
                  f'{height:.3f}', ha='center', va='bottom', fontsize=9)

lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax1_twin.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')

# ========== 2. Friedman Test Visualization ==========
ax2 = fig.add_subplot(gs[0, 2])

# Friedman test info
friedman_chi2 = 3.4746
friedman_p = 0.6272

ax2.text(0.5, 0.7, 'Friedman Test', fontsize=16, fontweight='bold',
         ha='center', transform=ax2.transAxes)
ax2.text(0.5, 0.5, f'χ² = {friedman_chi2:.4f}', fontsize=14,
         ha='center', transform=ax2.transAxes)
ax2.text(0.5, 0.35, f'P-value = {friedman_p:.4f}', fontsize=14,
         ha='center', transform=ax2.transAxes,
         color='green' if friedman_p > 0.05 else 'red')
ax2.text(0.5, 0.2, 'Not Significant', fontsize=12,
         ha='center', transform=ax2.transAxes,
         style='italic', color='green')
ax2.text(0.5, 0.05, '(p > 0.05)', fontsize=10,
         ha='center', transform=ax2.transAxes, color='gray')
ax2.axis('off')

# ========== 3. Mean Ranks from Friedman Test ==========
ax3 = fig.add_subplot(gs[1, :])

conditions = ['P1\nClinical', 'P1\nReflective', 'P2\nClinical',
              'P2\nReflective', 'P3\nClinical', 'P3\nReflective']
mean_ranks = [1.70, 2.20, 3.80, 3.90, 4.50, 4.90]

bars = ax3.barh(conditions, mean_ranks, color=colors, alpha=0.8, edgecolor='black')
ax3.set_xlabel('Mean Rank (Lower = Better Performance)', fontsize=12, fontweight='bold')
ax3.set_title('Friedman Test: Mean Ranks by Condition (N=10 subjects, k=6 conditions)',
              fontsize=14, fontweight='bold', pad=20)
ax3.grid(axis='x', alpha=0.3)

# Add value labels
for i, (bar, val) in enumerate(zip(bars, mean_ranks)):
    ax3.text(val + 0.1, bar.get_y() + bar.get_height()/2.,
             f'{val:.2f}', va='center', fontsize=11, fontweight='bold')

# Add vertical line at midpoint
ax3.axvline(x=3.5, color='gray', linestyle='--', alpha=0.5, linewidth=2)

# ========== 4. Sign Test Results ==========
ax4 = fig.add_subplot(gs[2, 0])

sign_test_data = {
    'First Pass': {'positive': 3, 'negative': 3, 'ties': 4},
    'Second Pass': {'positive': 1, 'negative': 0, 'ties': 9},
    'Third Pass': {'positive': 1, 'negative': 1, 'ties': 8}
}

passes_sign = list(sign_test_data.keys())
positives = [sign_test_data[p]['positive'] for p in passes_sign]
negatives = [sign_test_data[p]['negative'] for p in passes_sign]
ties = [sign_test_data[p]['ties'] for p in passes_sign]

x_pos = np.arange(len(passes_sign))
width = 0.25

bars1 = ax4.bar(x_pos - width, positives, width, label='Positive Diff', color='#2ecc71', alpha=0.8)
bars2 = ax4.bar(x_pos, negatives, width, label='Negative Diff', color='#e74c3c', alpha=0.8)
bars3 = ax4.bar(x_pos + width, ties, width, label='Ties', color='#95a5a6', alpha=0.8)

ax4.set_xlabel('Pass', fontsize=11, fontweight='bold')
ax4.set_ylabel('Count', fontsize=11, fontweight='bold')
ax4.set_title('Sign Test: Direction of Differences', fontsize=13, fontweight='bold', pad=15)
ax4.set_xticks(x_pos)
ax4.set_xticklabels(['First', 'Second', 'Third'])
ax4.legend()
ax4.grid(axis='y', alpha=0.3)

# Add value labels
for bars in [bars1, bars2, bars3]:
    for bar in bars:
        height = bar.get_height()
        if height > 0:
            ax4.text(bar.get_x() + bar.get_width()/2., height,
                     f'{int(height)}', ha='center', va='bottom', fontsize=9)

# ========== 5. P-values Summary Heat Map ==========
ax5 = fig.add_subplot(gs[2, 1])

p_values_summary = np.array([
    [1.000],  # First Pass Wilcoxon
    [1.000],  # Second Pass Wilcoxon
    [1.000],  # Third Pass Wilcoxon
    [0.6272], # Friedman
    [1.000],  # First Pass Sign
    [1.000],  # Second Pass Sign
    [1.000],  # Third Pass Sign
])

sns.heatmap(p_values_summary, annot=True, fmt='.4f', cmap='RdYlGn',
            vmin=0, vmax=1, cbar_kws={'label': 'P-value'},
            yticklabels=['Wilcoxon P1', 'Wilcoxon P2', 'Wilcoxon P3',
                        'Friedman', 'Sign P1', 'Sign P2', 'Sign P3'],
            xticklabels=['P-value'], ax=ax5, linewidths=2, linecolor='black')
ax5.set_title('P-values Summary\n(Green = Not Significant)', fontsize=13, fontweight='bold', pad=15)
ax5.axhline(y=0, color='red', linewidth=3, linestyle='--', alpha=0.7)
ax5.text(1.15, 0.5, 'α=0.05\nthreshold', transform=ax5.transData,
         fontsize=9, color='red', ha='left', va='center')

# ========== 6. Statistical Interpretation Summary ==========
ax6 = fig.add_subplot(gs[2, 2])

summary_text = """
STATISTICAL SUMMARY

All Tests: NOT SIGNIFICANT
(All p-values > 0.05)

Key Findings:
• Wilcoxon Tests: No pairwise
  differences between rubrics

• Friedman Test: No overall
  difference across 6 conditions

• Sign Tests: No directional
  preference (balanced +/-)

Interpretation:
Clinical vs Reflective rubrics
show NO statistically significant
differences in any comparison.

Results are CONSISTENT across
all three passes and all tests.
"""

ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes,
         fontsize=10, verticalalignment='top', family='monospace',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax6.axis('off')

# Main title
fig.suptitle('Comprehensive Statistical Analysis: Clinical vs Reflective Rubrics',
             fontsize=18, fontweight='bold', y=0.98)

plt.tight_layout()
plt.show()

# ========== Print Numerical Summary ==========
print("=" * 70)
print("STATISTICAL ANALYSIS SUMMARY")
print("=" * 70)
print("\n1. WILCOXON SIGNED-RANK TEST (Pairwise Comparisons)")
print("-" * 70)
for i, (pass_name, w, p) in enumerate(zip(passes, w_stats, p_values_wilcoxon), 1):
    print(f"{pass_name:15s}: W = {w:5.1f}, p = {p:.4f} → Not Significant")

print("\n2. FRIEDMAN TEST (Overall Comparison)")
print("-" * 70)
print(f"Chi-squared = {friedman_chi2:.4f}, p = {friedman_p:.4f} → Not Significant")
print("\nMean Ranks (lower rank indicates lower median score):")
for cond, rank in zip(conditions, mean_ranks):
    print(f"  {cond.replace(chr(10), ' '):20s}: {rank:.2f}")

print("\n3. SIGN TEST (Direction of Differences)")
print("-" * 70)
for pass_name, data in sign_test_data.items():
    print(f"{pass_name:15s}: +{data['positive']}, -{data['negative']}, "
          f"ties={data['ties']}, p = 1.0000 → Not Significant")

print("\n" + "=" * 70)
print("OVERALL CONCLUSION")
print("=" * 70)
print("No statistically significant differences were found between Clinical")
print("and Reflective rubrics across any test or any pass (all p > 0.05).")
print("The scoring systems appear to be equivalent in their assessment outcomes.")
print("=" * 70)